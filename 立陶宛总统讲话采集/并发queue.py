import asyncio
import os
import random
import sys
import time
import json
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import aiohttp
from loguru import logger
from lxml import html

from async_custom_mysql_pool import DbManager
from ç«‹é™¶å®›æ€»ç»Ÿè®²è¯é‡‡é›†.å¢é‡çˆ¬è™«_è·å–è¯·æ±‚é“¾æ¥ import cookies

# Windows å¹³å°å…¼å®¹æ€§è®¾ç½®
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

cmd = sys.argv
mod = 1
n = 0
if len(cmd) > 1:
    mod = int(cmd[1])
    n = int(cmd[2])

# å¹¶å‘é…ç½®
MAX_CONCURRENT_REQUESTS = 20
BATCH_SIZE = 50  # å‡å°‘æ‰¹é‡å¤§å°ï¼Œæ›´é¢‘ç¹æ’å…¥
REAL_TIME_INSERT_THRESHOLD = 10  # è¾¾åˆ°è¿™ä¸ªæ•°é‡å°±ç«‹å³æ’å…¥
MAX_RETRIES = 3
RETRY_DELAY = 2

headers = {
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "zh-CN,zh;q=0.9",
    "cache-control": "no-cache",
    "dnt": "1",
    "pragma": "no-cache",
    "priority": "u=0, i",
    "sec-ch-ua": "\"Google Chrome\";v=\"141\", \"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"141\"",
    "sec-ch-ua-arch": "\"x86\"",
    "sec-ch-ua-bitness": "\"64\"",
    "sec-ch-ua-full-version": "\"141.0.7390.123\"",
    "sec-ch-ua-full-version-list": "\"Google Chrome\";v=\"141.0.7390.123\", \"Not?A_Brand\";v=\"8.0.0.0\", \"Chromium\";v=\"141.0.7390.123\"",
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-model": "\"\"",
    "sec-ch-ua-platform": "\"Windows\"",
    "sec-ch-ua-platform-version": "\"15.0.0\"",
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "none",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36"
}
cookies = {
    "privacy_2": "0",
    "privacy_3": "0",
    "privacy_4": "0",
    "privacy_verify": "1",
    "cf_clearance": "9OspfTJUwgBfX0ppwWljCvG4zhJBbYE6285VV8FZM6c-1762478537-1.2.1.1-biYo2Gen.10fZPTG222gkgikXyOEyKZEfgAGREcdleMYs6ampFTPaxehFV2jr9rIJmtAl3PPtyMSSOZquZs7XCTkac9wJ.khepFv0T4cD90zQJncpftnOipAcTUJDUQ9Gf2yqHZLfSxcE71Hb27ZQMygmIDHv3PLA8_PpPDLwd7ZBVgkAzQPa2GREwkxl5SE8v4PVRNu60QVuo21Wd84temPDiOG0recBRHD4PtC9Do",
    "EW4SITE": "ccl554rjlv88trqcd2ktlg9nsc",
    "SITEXSRF141": "qhp7n3buuccx8r8b248ptt32rj1wd6zj"
}

# URLé…ç½®
add_url_list1_map = {
    "https://archyvas.lrp.lt/adamkus/sarasas15ee.html?nuo=1&kat=4&search=": "LT",
    "https://archyvas.lrp.lt/adamkus/en/sarasasdd13.html?nuo=1&kat=12&search=": "EN",
    "https://archyvas.lrp.lt/paksas/sarasas8710.html?kat=4": "LT",
    "https://archyvas.lrp.lt/paksas/en/sarasasdd13.html?nuo=1&kat=12&search=": "EN"
}

add_url_list2_map = {
    "https://grybauskaite.lrp.lt/en/activities/speeches/6590/2009-11": "EN",
    "https://grybauskaite.lrp.lt/lt/prezidentes-veikla/kalbos/6588/2023-06": "LT"
}

add_url_list3_map = {'https://archyvas.lrp.lt/adamkus3/en/activities/speeches/p15.html': "LT"}


@dataclass
class ArticleData:
    """æ–‡ç« æ•°æ®ç»“æ„"""
    url: str
    date: str
    lang: str
    source_type: str

    def to_dict(self) -> Dict[str, Any]:
        return {
            'url': self.url,
            'date': self.date,
            'lang': self.lang,
            'source_type': self.source_type,
        }

    def __str__(self) -> str:
        return f"Article({self.lang}-{self.source_type}): {self.date} - {self.url}"


class RealTimeSpider:
    def __init__(self, db_manager: DbManager, max_concurrent_requests: int = MAX_CONCURRENT_REQUESTS):
        self.db_manager = db_manager
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        self.session = None
        self.article_queue = asyncio.Queue()
        self.processed_count = 0
        self.start_time = time.time()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_articles': 0,
            'successful_inserts': 0,
            'failed_inserts': 0,
            'type1_count': 0,
            'type2_count': 0,
            'type3_count': 0
        }

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(
                limit=MAX_CONCURRENT_REQUESTS,
                ssl=False,
                force_close=True,
                enable_cleanup_closed=True
            ),
            timeout=aiohttp.ClientTimeout(total=30),
            headers=headers,
            cookies=cookies
        )

        # å¯åŠ¨å®æ—¶æ’å…¥ä»»åŠ¡
        asyncio.create_task(self._real_time_inserter())

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

        # æ’å…¥é˜Ÿåˆ—ä¸­å‰©ä½™çš„æ•°æ®
        await self._flush_queue()

    async def _real_time_inserter(self):
        """å®æ—¶æ’å…¥å™¨ï¼Œå®šæœŸæ£€æŸ¥å¹¶æ’å…¥é˜Ÿåˆ—ä¸­çš„æ•°æ®"""
        buffer = []

        while True:
            try:
                # ç­‰å¾…æ–°æ•°æ®æˆ–è¶…æ—¶
                article = await asyncio.wait_for(self.article_queue.get(), timeout=1.0)
                buffer.append(article)

                # è¾¾åˆ°é˜ˆå€¼æˆ–ç¼“å†²åŒºæœ‰ä¸€å®šæ•°é‡æ—¶ç«‹å³æ’å…¥
                if (len(buffer) >= REAL_TIME_INSERT_THRESHOLD or
                        self.article_queue.empty() and buffer):
                    await self._insert_batch(buffer)
                    buffer = []

            except asyncio.TimeoutError:
                # è¶…æ—¶ä½†ç¼“å†²åŒºæœ‰æ•°æ®ï¼Œæ’å…¥
                if buffer:
                    await self._insert_batch(buffer)
                    buffer = []
            except Exception as e:
                logger.error(f"å®æ—¶æ’å…¥å™¨é”™è¯¯: {e}")

    async def _insert_batch(self, articles: List[ArticleData]):
        """æ‰¹é‡æ’å…¥æ•°æ®"""
        if not articles:
            return

        try:
            data_to_insert = [article.to_dict() for article in articles]

            await self.db_manager.table_insert_batch(
                table='lt_news_queue',
                data=data_to_insert,
            )

            self.stats['successful_inserts'] += len(articles)

            # æ‰“å°æ’å…¥çš„æ•°æ®
            for article in articles:
                self._print_article(article)
                self.stats['total_articles'] += 1

            logger.info(f"âœ“ æˆåŠŸæ’å…¥ {len(articles)} ç¯‡æ–‡ç« åˆ°æ•°æ®åº“")

        except Exception as e:
            self.stats['failed_inserts'] += len(articles)
            logger.error(f"æ’å…¥å¤±è´¥: {e}")
            # å¯ä»¥é€‰æ‹©é‡è¯•æˆ–è®°å½•åˆ°æ–‡ä»¶

    async def _flush_queue(self):
        """æ¸…ç©ºé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®"""
        buffer = []
        while not self.article_queue.empty():
            try:
                article = self.article_queue.get_nowait()
                buffer.append(article)
            except asyncio.QueueEmpty:
                break

        if buffer:
            await self._insert_batch(buffer)

    def _print_article(self, article: ArticleData):
        """æ ¼å¼åŒ–æ‰“å°æ–‡ç« ä¿¡æ¯"""
        elapsed = time.time() - self.start_time
        rate = self.stats['total_articles'] / elapsed if elapsed > 0 else 0

        print("â”Œ" + "â”€" * 80 + "â”")
        print(f"â”‚ ğŸ“° æ–‡ç«  #{self.stats['total_articles'] + 1:04d} | é€Ÿç‡: {rate:.2f} ç¯‡/ç§’")
        print(f"â”‚ æ¥æº: {article.source_type:8} | è¯­è¨€: {article.lang:2} | æ—¥æœŸ: {article.date:15}")
        print(f"â”‚ é“¾æ¥: {article.url}")
        print("â””" + "â”€" * 80 + "â”˜")

    async def fetch_page_with_retry(self, url: str, retries: int = MAX_RETRIES) -> str:
        """å¸¦é‡è¯•æœºåˆ¶çš„é¡µé¢è·å–"""
        for attempt in range(retries):
            try:
                async with self.semaphore:
                    await asyncio.sleep(random.uniform(0.3, 0.8))

                    async with self.session.get(url) as response:
                        if response.status in (444, 418, 413):
                            raise Exception(f'åçˆ¬: HTTP {response.status}')

                        html_content = await response.text()

                        # åçˆ¬å†…å®¹æ£€æŸ¥
                        if 'Just a moment...' in html_content:
                            raise Exception(f'åçˆ¬ï¼šè¯·æ±‚å¤±è´¥ï¼ï¼ˆå†…å®¹è¿‡çŸ­ï¼‰URL: {url}')

                        return html_content

            except Exception as e:
                if attempt == retries - 1:
                    raise
                await asyncio.sleep(RETRY_DELAY * (attempt + 1))

        raise Exception(f'æ‰€æœ‰é‡è¯•å¤±è´¥: {url}')

    async def process_type1_urls(self):
        """å¤„ç†type1 URLs - é¡µé¢å¢é‡"""
        logger.info("ğŸš€ å¼€å§‹å¤„ç†Type1 URLs (é¡µé¢å¢é‡)")

        tasks = []
        for url, lang in add_url_list1_map.items():
            tasks.append(self._process_single_type1_site(url, lang))

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_single_type1_site(self, base_url: str, lang: str):
        """å¤„ç†å•ä¸ªtype1ç«™ç‚¹"""
        current_url = base_url
        page_count = 1
        site_base = base_url.split('sarasas')[0]

        while current_url:
            try:
                logger.debug(f"Type1-{lang} ç¬¬{page_count}é¡µ: {current_url}")
                html_content = await self.fetch_page_with_retry(current_url)
                tree = html.fromstring(html_content)

                # æå–æ–‡ç« æ•°æ®
                url_list = tree.xpath('//div[@align="justify"]//tbody//a/@href')
                date_list = tree.xpath('//div[@align="justify"]//tbody//td[1]/text()')

                articles = []
                for i, url_item in enumerate(url_list):
                    article = ArticleData(
                        url=site_base + url_item,
                        date=date_list[i],
                        lang=lang,
                        source_type='TYPE1',
                    )
                    articles.append(article)
                    self.stats['type1_count'] += 1

                # ç«‹å³æ”¾å…¥é˜Ÿåˆ—
                for article in articles:
                    await self.article_queue.put(article)

                logger.info(f"Type1-{lang} ç¬¬{page_count}é¡µ: æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

                # æ£€æŸ¥ä¸‹ä¸€é¡µ
                next_url_ele = tree.xpath('//a[.="Toliau >>"]/@href')
                current_url = site_base + next_url_ele[0] if next_url_ele else None
                page_count += 1

            except Exception as e:
                logger.error(f"Type1-{lang} é¡µé¢å¤„ç†å¤±è´¥: {e}")
                current_url = None

    async def process_type2_urls(self):
        """å¤„ç†type2 URLs - å¹´ä»½æœˆä»½å¢é‡"""
        logger.info("ğŸš€ å¼€å§‹å¤„ç†Type2 URLs (å¹´ä»½æœˆä»½å¢é‡)")

        tasks = []
        for base_url, lang in add_url_list2_map.items():
            tasks.append(self._process_single_type2_site(base_url, lang))

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_single_type2_site(self, base_url: str, lang: str):
        """å¤„ç†å•ä¸ªtype2ç«™ç‚¹"""
        site_base = 'https://grybauskaite.lrp.lt'
        ID = 6590 if lang == 'EN' else 6588

        # åˆ›å»ºæ‰€æœ‰å¹´ä»½æœˆä»½çš„ä»»åŠ¡
        year_tasks = []
        for year in range(2009, 2024):
            year_tasks.append(self._process_type2_year(year, lang, ID, site_base))

        # æ§åˆ¶å¹¶å‘ï¼Œé¿å…åŒæ—¶å¤„ç†å¤ªå¤šå¹´ä»½
        for i in range(0, len(year_tasks), 3):  # æ¯æ¬¡å¹¶å‘3ä¸ªå¹´ä»½
            batch = year_tasks[i:i + 3]
            await asyncio.gather(*batch, return_exceptions=True)
            await asyncio.sleep(1)  # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿ

    async def _process_type2_year(self, year: int, lang: str, site_id: int, site_base: str):
        """å¤„ç†å•ä¸ªå¹´ä»½"""
        month_tasks = []
        for month in range(1, 13):
            url = f'https://grybauskaite.lrp.lt/{lang.lower()}/activities/speeches/{site_id}/{year}-{month:02d}'
            month_tasks.append(self._process_type2_month(url, lang, site_base, year, month))

        # å¹¶å‘å¤„ç†æ‰€æœ‰æœˆä»½
        await asyncio.gather(*month_tasks, return_exceptions=True)

    async def _process_type2_month(self, url: str, lang: str, site_base: str, year: int, month: int):
        """å¤„ç†å•ä¸ªæœˆä»½"""
        try:
            logger.debug(f"Type2 å¤„ç†: {year}-{month:02d}")
            html_content = await self.fetch_page_with_retry(url)
            tree = html.fromstring(html_content)

            url_list = tree.xpath('//div[@class="news_list"]/div/a/@href')
            if not url_list:
                return

            date_list = tree.xpath('//div[@class="news_list"]/div/a/div/span[@class="date"]/text()')

            articles = []
            for i, url_item in enumerate(url_list):
                article = ArticleData(
                    url=site_base + url_item,
                    date=date_list[i] if i < len(date_list) else f"{year}-{month:02d}",
                    lang=lang,
                    source_type='TYPE2',
                )
                articles.append(article)
                self.stats['type2_count'] += 1

            # ç«‹å³æ”¾å…¥é˜Ÿåˆ—
            for article in articles:
                await self.article_queue.put(article)

            logger.info(f"Type2 {year}-{month:02d}: æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        except Exception as e:
            logger.error(f"Type2 {year}-{month:02d} å¤„ç†å¤±è´¥: {e}")

    async def process_type3_urls(self):
        """å¤„ç†type3 URLs"""
        logger.info("ğŸš€ å¼€å§‹å¤„ç†Type3 URLs")

        tasks = []
        site_base = 'https://archyvas.lrp.lt/adamkus3/en/activities/speeches/'

        for page in range(0, 16):
            url = f'https://archyvas.lrp.lt/adamkus3/en/activities/speeches/p{page}.html'
            tasks.append(self._process_single_type3_page(url, site_base, page))

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_single_type3_page(self, url: str, site_base: str, page_num: int):
        """å¤„ç†å•ä¸ªtype3é¡µé¢"""
        try:
            logger.debug(f"Type3 ç¬¬{page_num}é¡µ: {url}")
            html_content = await self.fetch_page_with_retry(url)
            tree = html.fromstring(html_content)

            url_list = tree.xpath('//div[@id="inner-container"]/div[@class="news_date_and_title"]/span/a/@href')
            date_list = tree.xpath('//div[@id="inner-container"]/div[@class="news_date_and_title"]/span[@class="news_date"]/text()')

            articles = []
            for i, url_item in enumerate(url_list):
                article = ArticleData(
                    url=site_base + url_item,
                    date=date_list[i] if i < len(date_list) else 'æœªçŸ¥æ—¥æœŸ',
                    lang='LT',
                    source_type='TYPE3',
                )
                articles.append(article)
                self.stats['type3_count'] += 1

            # ç«‹å³æ”¾å…¥é˜Ÿåˆ—
            for article in articles:
                await self.article_queue.put(article)

            logger.info(f"Type3 ç¬¬{page_num}é¡µ: æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        except Exception as e:
            logger.error(f"Type3 ç¬¬{page_num}é¡µå¤„ç†å¤±è´¥: {e}")

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        elapsed = time.time() - self.start_time
        rate = self.stats['total_articles'] / elapsed if elapsed > 0 else 0

        print("\n" + "=" * 80)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 80)
        print(f"æ€»è¿è¡Œæ—¶é—´: {elapsed:.2f} ç§’")
        print(f"å¤„ç†é€Ÿç‡: {rate:.2f} ç¯‡/ç§’")
        print(f"æ€»æ–‡ç« æ•°: {self.stats['total_articles']}")
        print(f"æˆåŠŸæ’å…¥: {self.stats['successful_inserts']}")
        print(f"æ’å…¥å¤±è´¥: {self.stats['failed_inserts']}")
        print(f"Type1æ–‡ç« : {self.stats['type1_count']}")
        print(f"Type2æ–‡ç« : {self.stats['type2_count']}")
        print(f"Type3æ–‡ç« : {self.stats['type3_count']}")
        print("=" * 80)


async def main_async():
    """ä¸»å¼‚æ­¥å‡½æ•°"""
    async with DbManager(config=0) as db_manager:
        db_manager.fsign = 1

        async with RealTimeSpider(db_manager) as spider:
            try:
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰URLç±»å‹å¤„ç†
                logger.info("å¼€å§‹å¹¶å‘çˆ¬å–æ‰€æœ‰URLç±»å‹...")

                await asyncio.gather(
                    spider.process_type1_urls(),
                    spider.process_type2_urls(),
                    spider.process_type3_urls(),
                    return_exceptions=True
                )

                # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å·²æ’å…¥
                await asyncio.sleep(2)

                # æ‰“å°æœ€ç»ˆç»Ÿè®¡
                spider.print_stats()

            except Exception as e:
                logger.error(f"ä¸»æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
                raise
            finally:
                # æœ€ç»ˆç»Ÿè®¡
                spider.print_stats()


if __name__ == "__main__":
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)